import pandas as pd
import nltk
import re
import numpy as np
# Used for confusion model which is to tell you how accurate your model is
from sklearn import metrics
# Removing filler words
from nltk.corpus import stopwords
# Simplifying words to base value
from nltk.stem import WordNetLemmatizer
# Train test split (66% train - 33% test)
from sklearn.model_selection import train_test_split
# Used for making str into int (kind of)
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection  import KFold, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer


# Logistic Regression model computes probability of an outcome based on an input variable(s)
from sklearn.linear_model import LogisticRegression
df1 = pd.read_csv(r'C:\\Users\keena\\Desktop\\ML & AI\\militaria_products_data.csv',delimiter='|')

df = df1.loc[(df1['conflict'].notnull())&(df1['nation'].notnull())&(df1['item_type'].notnull())]
# df1.dropna(subset=['conflict','nation','item_type']) I think this does the same thing but in place.

df['text'] = df['title'] + ' ' + df['description']
data = df[['text','conflict','nation','item_type']]

print(data['text'].isna().sum())
print(data['item_type'].isna().sum())
print(df1.isna().sum().sort_values())

text = list(data['text'].dropna())
# Preprocess the text / data
lemmatizer = WordNetLemmatizer()
# Corpus is used when having a list of lists used for analysis
corpus = []
#Condensing the text to only key components
for i in range(len(text)):
    # Replace any non letters with ' ' with text[i] being the current iteration
    r = re.sub('[^a-zA-Z]',' ',text[i])
    # Make everything lowercase
    r = r.lower()
    # Split every word into an item in a list
    r = r.split()
    # Remove all stopwords / filler words
    r = [word for word in r if word not in stopwords.words('english')]
    # Reduce words to base word
    r = [lemmatizer.lemmatize(word) for word in r]
    # Reconnect all the words in a list back into one list item
    r = ' '.join(r)
    # Put this singular list with all the cleaned words into 'corpus'
    corpus.append(r)
data['text'] = corpus
data['text'].head()

# Independent Variable, what you have.
X = data['text']
# Dependent Variable, what you want.
y  = data['item_type']
y2 = data['conflict']
y3 = data['nation']

# Split the data to train / test
X_train, X_test, y_train, y_test   = train_test_split(X, y, test_size=0.33, random_state=111)
X_train, X_test, y2_train, y2_test = train_test_split(X, y2, test_size=0.33, random_state=111)
X_train, X_test, y3_train, y3_test = train_test_split(X, y3, test_size=0.33, random_state=111)

cv = CountVectorizer()
lr = LogisticRegression()
lr2 = LogisticRegression()
lr3 = LogisticRegression()

vectorizer = TfidfVectorizer()

X_train_cv = cv.fit_transform(X_train)

lr.fit(X_train_cv,y_train)
lr2.fit(X_train_cv,y2_train)
lr3.fit(X_train_cv,y3_train)

# Convert all the data sets into numeric values for ML to read.
X_test_cv = cv.transform(X_test)

X_train_cv.shape

predictions = lr.predict(X_test_cv)

predictions

# This prints out an easy to understand report of how good the model is.
print (classification_report(y_test, predictions))
# This is to see how accurate our prediction model is.

kf = KFold(n_splits=8, shuffle=True, random_state=5)
cv_scores = cross_val_score(lr, X_test_cv, y_test, cv=kf)

# Print the mean
print(np.mean(cv_scores))

# Print the standard deviation
print(np.std(cv_scores))

# Print the 95% confidence interval
print(np.quantile(cv_scores, [0.025, 0.975]))

pdData = {'testText': ["""



GERMAN WOOD SOUVENIR PLATE
German Wood Souvenir Plate-This colorful wooden plate measures 15 inches in diameter. It is hand carved, and hand painted around the edges with multiple crests for German cities. The center of the plate shows a nice image of Frankfurt. There is some wear to the paint, but mostly on the black trim that edges this plate. Otherwise, this is in very good condition. The reverse has a small hook with a piece of cord attached for hanging. We have no idea of its’ vintage and are selling this “as is”, it is a nice display piece.
"""]}

tdf = pd.DataFrame(pdData)

text = list(tdf['testText'].dropna())
# Preprocess the text / data
lemmatizer = WordNetLemmatizer()
# Corpus is used when having a list of lists used for analysis
corpus2 = []
#Condensing the text to only key components
for i in range(len(text)):
    # Replace any non letters with ' ' with text[i] being the current iteration
    r = re.sub('[^a-zA-Z]',' ',text[i])
    # Make everything lowercase
    r = r.lower()
    # Split every word into an item in a list
    r = r.split()
    # Remove all stopwords / filler words
    r = [word for word in r if word not in stopwords.words('english')]
    # Reduce words to base word
    r = [lemmatizer.lemmatize(word) for word in r]
    # Reconnect all the words in a list back into one list item
    r = ' '.join(r)
    # Put this singular list with all the cleaned words into 'corpus'
    corpus2.append(r)
tdf['testText'] = corpus2
print(tdf['testText'].head())

tdfTest = tdf['testText']

tdfTest_cv = cv.transform(tdfTest)


itemTypePrediction = lr.predict(tdfTest_cv)
conflictPrediction = lr2.predict(tdfTest_cv)
nationPrediction   = lr3.predict(tdfTest_cv)

print(f'ITEM_TYPE PREDICTION :{itemTypePrediction}')
print(f'CONFLICT PREDICTION  :{conflictPrediction}')
print(f'NATION PREDICTION    :{nationPrediction}')
